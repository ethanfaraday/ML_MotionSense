# -*- coding: utf-8 -*-
"""MotionSenseFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10dHX9Pxk4QUUK1NJjLxoSncZ7MrF51pd
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

def get_ds_infos():
    dss = pd.read_csv("./data/data_subjects_info.csv")
    print("[INFO] -- Data subjects' information is imported.")
    return dss

def set_data_types(data_types=["userAcceleration"]):
    dt_list = []
    for t in data_types:
        if t != "attitude":
            dt_list.append([t+".x",t+".y",t+".z"])
        else:
            dt_list.append([t+".roll", t+".pitch", t+".yaw"])
    return dt_list

def create_time_series(dt_list, act_labels, trial_codes, mode="mag", labeled=True):
    num_data_cols = len(dt_list) if mode == "mag" else len(dt_list*3)
    dataset = np.zeros((0,num_data_cols+1)) if labeled else np.zeros((0,num_data_cols))
    ds_list = get_ds_infos()
    print("[INFO] -- Creating Time-Series")
    for sub_id in ds_list["code"]:
        for act_id, act in enumerate(act_labels):
            for trial in trial_codes[act_id]:
                fname = f'./data/A_DeviceMotion_data/{act}_{trial}/sub_{int(sub_id)}.csv'
                raw_data = pd.read_csv(fname).drop(['Unnamed: 0'], axis=1)
                vals = np.zeros((len(raw_data), num_data_cols))
                for x_id, axes in enumerate(dt_list):
                    if mode == "mag":
                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5
                    else:
                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values
                    vals = vals[:,:num_data_cols]
                if labeled:
                    lbls = np.array([[act_id]]*len(raw_data))
                    vals = np.concatenate((vals, lbls), axis=1)
                dataset = np.append(dataset, vals, axis=0)
    cols = [col for axes in dt_list for col in (axes if mode=="raw" else [axes[0][:-2]])]
    if labeled: cols += ["act"]
    return pd.DataFrame(data=dataset, columns=cols)

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/mmalekzadeh/motion-sense.git
!unzip /content/motion-sense/data/A_DeviceMotion_data.zip -d /content/motion-sense/data/
!unzip /content/motion-sense/data/B_Accelerometer_data.zip -d /content/motion-sense/data/
!unzip /content/motion-sense/data/B_Gyroscope_data.zip -d /content/motion-sense/data/
!cp /content/motion-sense/data_subjects_info.csv /content/motion-sense/data/
# %cd /content/motion-sense/

ACT_LABELS = ["dws","ups", "wlk", "jog", "std", "sit"]
TRIAL_CODES = {
    ACT_LABELS[0]:[1,2,11],
    ACT_LABELS[1]:[3,4,12],
    ACT_LABELS[2]:[7,8,15],
    ACT_LABELS[3]:[9,16],
    ACT_LABELS[4]:[6,14],
    ACT_LABELS[5]:[5,13]
}

sdt = ["attitude","rotationRate" ,"userAcceleration"]
print("[INFO] -- Selected sensor data types:", sdt)
act_labels = ACT_LABELS[0:4]
print("[INFO] -- Selected activities:", act_labels)
trial_codes = [TRIAL_CODES[act] for act in act_labels]
dt_list = set_data_types(sdt)
dataset = create_time_series(dt_list, act_labels, trial_codes, mode="raw", labeled=True)
print("[INFO] -- Shape of time-series dataset:", dataset.shape)
dataset.head()

from sklearn.model_selection import TimeSeriesSplit
 ts_cv = TimeSeriesSplit(
    n_splits=5,
    gap=20,
    max_train_size=10000,
    test_size=1000
)

# Prepare flat data for classical models
X_flat = dataset.iloc[:, :-1].values
y_flat = dataset.iloc[:, -1].values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_flat)

X_train_flat, X_test_flat, y_train_flat, y_test_flat = train_test_split(X_flat, y_encoded, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_flat = scaler.fit_transform(X_train_flat)
X_test_flat = scaler.transform(X_test_flat)

# KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_flat, y_train_flat)
y_pred_knn = knn.predict(X_test_flat)
print("KNN Accuracy:", accuracy_score(y_test_flat, y_pred_knn))
print(classification_report(y_test_flat, y_pred_knn, target_names=[str(cls) for cls in label_encoder.classes_]))

# Logistic Regression
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_flat, y_train_flat)
y_pred_lr = log_reg.predict(X_test_flat)
print("Logistic Regression Accuracy:", accuracy_score(y_test_flat, y_pred_lr))
print(classification_report(y_test_flat, y_pred_lr, target_names=[str(cls) for cls in label_encoder.classes_]))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
# Separate features and labels
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

# Convert labels to numeric values
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)



# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)



# Predict and evaluate
y_pred_knn = knn.predict(X_test)
print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))

# Fix: convert label names to string
target_names = [str(cls) for cls in label_encoder.classes_]
print(classification_report(y_test, y_pred_knn, target_names=target_names))

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Train logistic regression
log_reg = LogisticRegression(max_iter=10000)
log_reg.fit(X_train, y_train)

# Predict and evaluate
y_pred_lr = log_reg.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))

# Convert label names to strings (fixes error)
target_names = [str(cls) for cls in label_encoder.classes_]
print(classification_report(y_test, y_pred_lr, target_names=target_names))

def windows(dataset, window_size=400, stride=200):
    windows_list = []
    labels = []
    for i in range(0, len(dataset) - window_size + 1, stride):
        window = dataset.iloc[i:i+window_size, :-1].values
        label = dataset.iloc[i + window_size - 1, -1]
        windows_list.append(window)
        labels.append(label)
    return pd.DataFrame({'X': list(windows_list), 'Y': labels})

windowed_df = windows(dataset, window_size=400, stride=200)
X = windowed_df["X"]
Y = windowed_df["Y"]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

class MotionSense(Dataset):
    def __init__(self, X, Y, transform=None):
        self.X = torch.tensor(np.stack(X.values), dtype=torch.float32)
        self.Y = torch.tensor(Y.values, dtype=torch.long)
        self.transform = transform
    def __getitem__(self, index):
        return self.X[index], self.Y[index]
    def __len__(self):
        return len(self.X)

train_set = MotionSense(X_train, Y_train)
test_set = MotionSense(X_test, Y_test)
train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
test_loader = DataLoader(test_set, batch_size=32)

class CNNNet(nn.Module):
    def __init__(self, in_channels, num_classes):
        super(CNNNet, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5)
        self.pool = nn.MaxPool1d(2)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=5)

        self._to_linear = None
        self.fc1 = None  # We'll define this later
        self.fc2 = nn.Linear(100, num_classes)

    def forward(self, x):
        x = x.permute(0, 2, 1)  # [batch, features, time]
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))

        if self._to_linear is None:
            self._to_linear = x.view(x.size(0), -1).shape[1]
            self.fc1 = nn.Linear(self._to_linear, 100).to(x.device)

        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        return self.fc2(x)

num_features = train_set.X.shape[2]
num_classes = len(np.unique(Y))
model = CNNNet(in_channels=num_features, num_classes=num_classes)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    for X_batch, y_batch in train_loader:
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += y_batch.size(0)
        correct += predicted.eq(y_batch).sum().item()
    print(f"Epoch {epoch+1}: Loss={running_loss:.3f} | Accuracy={100 * correct / total:.2f}%")

# Final test accuracy
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        outputs = model(X_batch)
        _, predicted = outputs.max(1)
        total += y_batch.size(0)
        correct += predicted.eq(y_batch).sum().item()
print(f"Test Accuracy: {100 * correct / total:.2f}%")

# K-fold validation
from sklearn.model_selection import KFold
kf = KFold(n_splits=2)
X_test=kf.split(X_test)

kf = KFold(n_splits=10,shuffle=False, random_state=None)
fold_results = []

for fold, (train_idx, test_idx) in enumerate(kf.split(X), start=1):
    print(f"\n=== Fold {fold} ===")

    # 1) Slice out this fold's train & test sets
    X_train_fold, X_test_fold = X[train_idx], X[test_idx]
    Y_train_fold, Y_test_fold = Y[train_idx], Y[test_idx]

    # 2) Build Datasets & DataLoaders
    train_set  = MotionSense(X_train_fold, Y_train_fold)
    test_set   = MotionSense(X_test_fold,  Y_test_fold)
    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
    test_loader  = DataLoader(test_set,  batch_size=32, shuffle=False)

    # 3) Init a fresh model & optimizer for each fold
    num_features = train_set.X.shape[2]
    num_classes  = len(np.unique(Y))
    model = CNNNet(in_channels=num_features, num_classes=num_classes)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 4) Train
    for epoch in range(1, num_epochs+1):
        model.train()
        running_loss = 0.0
        correct = 0
        total   = 0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss    = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()*y_batch.size(0)
            _, preds = outputs.max(1)
            correct  += preds.eq(y_batch).sum().item()
            total    += y_batch.size(0)

        epoch_loss = running_loss / total
        epoch_acc  = correct / total * 100
        print(f"Epoch {epoch:2d} — Loss: {epoch_loss:.3f}, Acc: {epoch_acc:.1f}%")

    # 5) Evaluate on this fold’s test split
    model.eval()
    correct = 0
    total   = 0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            _, preds = outputs.max(1)
            correct += preds.eq(y_batch).sum().item()
            total   += y_batch.size(0)

    fold_acc = 100 * correct / total
    print(f"Fold {fold} Test Accuracy: {fold_acc:.2f}%")
    fold_results.append(fold_acc)

# 6) Summarize cross-validation
import numpy as np
print(f"\nMean CV Accuracy: {np.mean(fold_results):.2f}% ± {np.std(fold_results):.2f}%")

# Required imports
import numpy as np
import pandas as pd
from sklearn.preprocessing  import LabelEncoder, StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model   import LogisticRegression
from sklearn.neighbors      import KNeighborsClassifier
from sklearn.metrics        import accuracy_score, classification_report

# 0) Prepare X, y
# Assume `dataset` is your DataFrame and the last column is the activity label
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

# 1) Encode labels and force them to strings for reports
label_encoder = LabelEncoder()
y_encoded    = label_encoder.fit_transform(y)
# Convert class names to strings so classification_report can call len()
target_names = [str(cls) for cls in label_encoder.classes_]

# 2) Set up stratified k-fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 3) Hold per-fold accuracies
lr_accuracies  = []
knn_accuracies = []

# 4) Loop over folds
for fold, (train_idx, test_idx) in enumerate(skf.split(X, y_encoded), start=1):
    print(f"\n=== Fold {fold} ===")

    # a) Slice train/test
    X_train_fold, X_test_fold = X[train_idx], X[test_idx]
    y_train_fold, y_test_fold = y_encoded[train_idx], y_encoded[test_idx]

    # b) Scale features per-fold
    scaler       = StandardScaler()
    X_train_fold = scaler.fit_transform(X_train_fold)
    X_test_fold  = scaler.transform(X_test_fold)

    # c) Logistic Regression
    log_reg = LogisticRegression(max_iter=1000)
    log_reg.fit(X_train_fold, y_train_fold)
    y_pred_lr = log_reg.predict(X_test_fold)
    acc_lr = accuracy_score(y_test_fold, y_pred_lr)
    lr_accuracies.append(acc_lr)
    print(f"Logistic Regression Accuracy: {acc_lr:.4f}")
    print(classification_report(
        y_test_fold,
        y_pred_lr,
        labels=np.arange(len(target_names)),
        target_names=target_names,
        zero_division=0
    ))

    # d) K-Nearest Neighbors
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train_fold, y_train_fold)
    y_pred_knn = knn.predict(X_test_fold)
    acc_knn = accuracy_score(y_test_fold, y_pred_knn)
    knn_accuracies.append(acc_knn)
    print(f"KNN Accuracy: {acc_knn:.4f}")
    print(classification_report(
        y_test_fold,
        y_pred_knn,
        labels=np.arange(len(target_names)),
        target_names=target_names,
        zero_division=0
    ))

# 5) Summarize cross-validation
print(f"\nAverage Logistic Regression CV Accuracy: {np.mean(lr_accuracies):.4f} ± {np.std(lr_accuracies):.4f}")
print(f"Average KNN CV Accuracy:                     {np.mean(knn_accuracies):.4f} ± {np.std(knn_accuracies):.4f}")

import matplotlib.pyplot as plt

# Replace these with your actual lists:

model_names = ['CNN', 'KNN', 'Logistic Regression']
def to_percent(lst):
    if max(lst) <= 1.0:
        return [x*100 for x in lst]
    else:
        return lst
knn_accuracies=to_percent(knn_accuracies)
lr_accuracies=to_percent(lr_accuracies)
accuracies = [fold_results, knn_accuracies, lr_accuracies]
plt.figure(figsize=(12, 6), dpi=100)

# 2) Draw the boxplot, showing means:
bp = plt.boxplot(accuracies, labels=model_names, showmeans=True)

# 3) (Optional) Increase font sizes for readability:
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.xlabel('Model', fontsize=16)
plt.ylabel('Accuracy (%)', fontsize=16)
plt.title('Model Accuracy Distributions (10-Fold CV)', fontsize=18)

# 4) (Optional) Add a light grid:
plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.tight_layout()
plt.show()

from sklearn.pipeline     import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors    import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
tscv = TimeSeriesSplit(n_splits=5)

# 1) Tune k for KNN
knn_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('knn',    KNeighborsClassifier())
])
k_val = [3,5,7]
best_k     = None
best_score = 0.0
for k in k_val:                # try  k from 5 to 10
    knn_pipe.set_params(knn__n_neighbors=k)
    scores = cross_val_score(
        knn_pipe,
        X_train_flat, y_train_flat,
        cv=tscv,
        scoring='accuracy'
    )
    mean_score = scores.mean()
    print(f'KNN: k={k:2d} → CV accuracy = {mean_score:.3f}')
    if mean_score > best_score:
        best_score = mean_score
        best_k     = k

print(f'\n>> Best KNN k = {best_k}, CV accuracy = {best_score:.3f}\n')


# 2) Tune C for Logistic Regression
lr_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('lr',     LogisticRegression(max_iter=10_000))
])
C_val = [0.1,1,10]
best_C      = None
best_score_lr = 0.0
for C in C_val:
    lr_pipe.set_params(lr__C=C)
    scores = cross_val_score(
        lr_pipe,
        X_train_flat, y_train_flat,
        cv=tscv,
        scoring='accuracy'
    )
    mean_score = scores.mean()
    print(f'LogReg: C={C:>5} → CV accuracy = {mean_score:.3f}')
    if mean_score > best_score_lr:
        best_score_lr = mean_score
        best_C        = C

print(f'\n>> Best LogReg C = {best_C}, CV accuracy = {best_score_lr:.3f}')



